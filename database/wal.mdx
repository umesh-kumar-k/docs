## Interview Summary
Write‑ahead logging (WAL) records every change in an append‑only log before mutating main storage, giving strong durability and crash recovery while allowing fast sequential disk writes. Senior architects should explain how WAL underpins ACID, replication, and resilience in systems like PostgreSQL, Kafka, MongoDB, RocksDB, and Netflix’s WAL‑as‑a‑Service, and reason about trade‑offs between durability, latency, throughput, and complexity.[1][2][3][4]

***

## Keywords & Patterns

- **Core concept**
  - “Log first, apply later”: append change records to a durable log, then apply to data files or in‑memory structures.[2][1]
  - Append‑only, sequential I/O → fewer random writes, better throughput, and simpler recovery.[5][1]
  - Guarantees: durability and crash consistency for committed changes.[6][1]

- **WAL in common systems**
  - PostgreSQL: WAL segments (e.g., 16 MB) used for crash recovery and streaming replication.[7][6]
  - MongoDB: oplog is a logical WAL feeding replication and recovery.[1]
  - RocksDB/LSM stores: WAL + memtables; WAL replays to rebuild memtables on restart.[3][5]
  - Distributed platforms: Netflix’s WAL service sits in front of multiple data stores (Cassandra, Redis, etc.) to capture all mutations first.[8][4][6]

- **Patterns built on WAL**
  - Crash recovery (redo/undo depending on design).[2][6]
  - Replication: followers read WAL/oplog/commit log and apply changes in order.[7][1]
  - Event sourcing / CQRS: WAL as “source of truth” log, with views/indexes as projections.[9][10]
  - Delayed retries, DLQs, and cross‑region replication by replaying log entries later or elsewhere.[4][8]

***

## Common Trade-offs + Example Questions

### Trade-offs

- **Durability vs latency**
  - Flushing WAL on every commit → strongest durability but higher write latency.[5][6]
  - Group commit / batched fsync → better throughput, slightly higher risk window for data loss.[5][7]

- **Simplicity vs features**
  - Physical WAL (page/byte‑level changes) → simple replay, larger logs.[2]
  - Logical WAL (row/statement changes) → easier cross‑version replication and downstream consumption, but more complex and potentially slower to apply.[1][7]

- **Storage overhead vs observability**
  - Longer retention makes backfill/recovery and auditing easier but costs disk and I/O; aggressive pruning saves space but reduces replay window and observability.[7][5]

- **Central WAL service vs per‑DB WAL**
  - Central service (Netflix) simplifies resilience and cross‑store consistency but introduces extra hop, new failure domain, and design complexity.[11][4][6]

### Example interview questions

- Explain how WAL guarantees durability and how a database uses WAL during crash recovery.[6][1][2]
- In a system with strict RPO=0 requirements, how would you configure WAL flushing and what is the impact on latency and throughput?[5][7]
- How can you use a WAL to implement replication and backfill for analytics or search indices?[4][1]
- Netflix built a WAL‑as‑a‑Service in front of heterogeneous data stores; what problems does this solve and what new risks does it introduce?[8][11][4]

***

## Use Cases

- **Traditional relational DBs (PostgreSQL, MySQL, Oracle‑style)**  
  - Use WAL for crash recovery (redo after crash), point‑in‑time recovery, and streaming replication to standbys.[6][2][7]

- **Document and key‑value stores (MongoDB, RocksDB‑based systems)**  
  - MongoDB’s oplog and RocksDB’s WAL hold operation sequences to recover in‑memory state and drive replication or backup.[3][1][5]

- **Streaming / distributed platforms**
  - Netflix WAL service: durable commit log for database mutations, delayed queues, cross‑region replication, and multi‑table/multi‑partition mutations with ordered replay.[12][11][8][4]

- **Event‑driven microservices**
  - Use WAL as authoritative event log, with idempotent consumers rebuilding projections, caches, and search indices by replaying the log.[10][9]

***

## Big Tech / Engineering Blog References

- **Architecture Weekly (Postgres, Kafka, MongoDB examples)**  
  - Explains how WAL underpins PostgreSQL crash recovery, Kafka’s commit log design, and MongoDB’s oplog‑based replication, positioning WAL as a foundational pattern across systems.[1]

- **Netflix – “Building a Resilient Data Platform with Write‑Ahead Log”**  
  - Describes a generic WAL service that logs all mutations first, then fans out to multiple backends (Cassandra, Redis, EVCache, etc.), enabling delayed retries, cross‑region replication, and multi‑partition operations with strong traceability and replay.[12][11][8][4]

- **RocksDB (Meta / Facebook)**  
  - Documents how every update goes to an in‑memory memtable and a WAL, which is later discarded after SST flush, ensuring crash consistency for embedded key‑value workloads.[13][3]

***

## Cheat‑sheet, Q&A Style

- **Q: What is a write‑ahead log (WAL)?**  
  - A: An append‑only, durable log where all changes are written before they are applied to the main data store, enabling durability and recovery.[2][1]

- **Q: How does WAL enable crash recovery?**  
  - A: On restart, the system scans the WAL and replays committed operations (and possibly undoes partial ones, depending on recovery algorithm) to reach a consistent state.[6][2]

- **Q: Why is WAL usually sequential?**  
  - A: Sequential appends exploit disk and SSD write patterns for higher throughput and lower latency than random in‑place updates.[1][5]

- **Q: What is the relationship between WAL and replication?**  
  - A: Followers or downstream systems read the WAL/commit log and apply entries in order, keeping replicas or projections up to date.[4][7][1]

- **Q: Physical vs logical WAL?**  
  - A: Physical logs data‑page changes; logical logs high‑level operations (row/statement), easier for cross‑version replication but with more complex re‑application.[2][1]

- **Q: What are common sync strategies?**  
  - A: Sync every write (max durability, higher latency), group commit (batch fsync), or background fsync with slightly higher loss window but better throughput.[7][5]

- **Q: How did Netflix extend WAL beyond databases?**  
  - A: Built WAL‑as‑a‑Service that captures all mutations in a durable log first, then routes to multiple data stores with retries, DLQs, and two‑phase style multi‑partition commits.[11][12][4]

- **Q: How is WAL different from a message queue?**  
  - A: Conceptually similar (append‑only log), but WAL is tightly coupled to storage engine semantics and recovery; queues may not guarantee full DB‑style atomicity or ordering guarantees per key without extra logic.[9][5]

***

## Data Structures & Algorithms Used

- **Data structures**
  - Append‑only log files segmented into fixed‑size WAL segments/frames.[5][7][6]
  - In‑memory buffers for batching and group commit before fsync.[6][5]
  - Log sequence numbers (LSN) / offsets / timestamps to uniquely identify and order records.[7][2]
  - Indexes or metadata for quick seek to a given LSN during recovery or replication.[13][6]

- **Algorithms**
  - Log‑structured write path: append change, flush (possibly batched), then apply to data files or memtable.[3][1][5]
  - Recovery scanning: scan from last checkpoint/redo‑point, replaying or undoing entries as required (often ARIES‑style analysis/redo/undo in full DBMS).[2][6]
  - Replication streaming: readers tail the WAL, track last applied LSN, and resume from there after failures.[3][7]
  - Retention and compaction: archive/purge older WAL segments once all relevant data is durable elsewhere and replicas are caught up.[13][5][7]
  - Netflix’s two‑phase‑like multi‑partition protocol using WAL entries + completion markers to ensure atomicity across multiple mutations.[12][11][4]

***

## Tools / Frameworks / Software

- **PostgreSQL WAL**
  - Native write‑ahead logging used for crash recovery, streaming replication, and point‑in‑time recovery; WAL archiving and WAL shipping are standard DR patterns.[7][6]

- **MongoDB oplog**
  - Replica‑set operation log acting as logical WAL; secondaries continuously tail this log for replication.[1]

- **RocksDB WAL**
  - Embedded key‑value engine: writes go to memtable and WAL; WAL files are used to rebuild memtables on restart and are purged after compaction.[13][3]

- **Kafka / commit‑log systems**
  - Used as external WAL/commit log for event sourcing, CDC pipelines, and Netflix’s WAL‑as‑a‑Service backend.[4][12][1]

- **Netflix WAL‑as‑a‑Service**
  - Managed platform providing durable log abstraction on top of Kafka/SQS for all data mutations across multiple stores and regions.[8][11][4]

***

## Concise Summary: Important Terms & Keywords

- Write‑ahead logging (WAL), append‑only log, “log first, apply later”.[1][2]
- Durability, crash consistency, ACID, redo/undo, recovery.[6][2]
- WAL segment, LSN (log sequence number), checkpoint, archive/prune.[7][6]
- Physical vs logical WAL, commit log, oplog, CDC.[2][1][7]
- Group commit, fsync policy, durability vs latency trade‑off.[5][7]
- Replication from WAL, streaming replication, point‑in‑time recovery.[6][7]
- RocksDB WAL + memtable, LSM trees, replay.[3][5]
- Netflix WAL‑as‑a‑Service, DLQ, retries, cross‑region replication, multi‑partition mutations.[11][8][4]

[1](https://www.architecture-weekly.com/p/the-write-ahead-log-a-foundation)
[2](https://en.wikipedia.org/wiki/Write-ahead_logging)
[3](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-(WAL))
[4](https://devblogs.sh/posts/building-a-resilient-data-platform-with-write-ahead-log-at-netflix)
[5](https://dbfromzero.com/write_ahead_log.html)
[6](https://www.postgresql.org/docs/8.0/wal.html)
[7](https://www.postgresql.fastware.com/blog/understanding-postgresql-write-ahead-logging-wal)
[8](https://www.infoq.com/news/2025/10/netflix-wal-resilience/)
[9](https://martinfowler.com/articles/patterns-of-distributed-systems/write-ahead-log.html)
[10](https://vivekbansal.substack.com/p/database-internals-write-ahead-logging)
[11](https://www.infoq.com/presentations/netflix-write-ahead-logging/)
[12](https://www.linkedin.com/posts/ram-chavali_how-netflix-built-a-resilient-data-platform-activity-7393170674703360000-xwJV)
[13](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-File-Format)
[14](https://hevodata.com/learn/write-ahead-logging/)
[15](https://www.reddit.com/r/databasedevelopment/comments/179xslr/things_to_keep_in_mind_when_creating_a/)
[16](https://www.geeksforgeeks.org/operating-systems/journaling-or-write-ahead-logging/)
[17](https://www.linkedin.com/posts/prabhu-ravichandran-r_dataplatforms-distributedsystems-cloudarchitecture-activity-7377450206407307264-nmeW)
[18](https://www.youtube.com/watch?v=poLBRs0kF08)
[19](https://app.studyraid.com/en/read/11970/381973/write-ahead-log-wal-architecture)
[20](https://www.youtube.com/watch?v=yV_Zp0Mi3xs)