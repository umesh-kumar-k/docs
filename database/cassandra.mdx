## Interview Summary
Cassandra is a distributed, wide‑column NoSQL database optimized for high write throughput, linear horizontal scalability, and always‑on availability across data centers, using a partition+clustering key data model over an LSM‑tree storage engine (commit log + memtables + SSTables). It excels when you can design query‑driven schemas around partition keys, tolerate eventual consistency, and need multi‑region, fault‑tolerant storage, but is a poor fit for ad‑hoc querying, complex joins, or strongly consistent cross‑row transactions.[1][2][3]

***

## Keywords & Patterns

- **Data model**
  - Keyspace → tables → rows → columns; keyspace defines replication strategy (SimpleStrategy, NetworkTopologyStrategy).[1]
  - Primary key = partition key (1+ columns) + optional clustering keys (sorted order within partition).[1]
  - Wide‑column: different rows can have different columns; columns carry timestamps, and conflicts use “last‑write‑wins”.[1]

- **Primary/partition/clustering keys**
  - Partition key: decides which node/partition stores the row and defines data locality.[1]
  - Clustering key(s): define on‑disk sort order inside a partition (e.g., messages by `message_id DESC`).[1]
  - Composite partition keys to avoid over‑large partitions (e.g., `(channel_id, bucket)` or `(event_id, section_id)`).[1]

- **Core concepts**
  - Partitioning: hash‑based distribution of partitions across nodes with virtual nodes to reduce rebalancing pain and hotspots.[4][1]
  - Replication: configurable replication factor per DC; strategies: `SimpleStrategy`, `NetworkTopologyStrategy`.[1]
  - Consistency: tunable per operation via CL (ONE, QUORUM, ALL, LOCAL_*), trading latency vs consistency.[2][1]
  - Query routing: client driver uses partitioner + token ring metadata (gossip) to route to correct replicas.[1]

- **Storage model**
  - Commit log (WAL) → memtable (in‑memory, sorted) → SSTables (immutable, sorted files on disk).[5][1]
  - Compaction merges SSTables, applies tombstones, and removes deleted/obsolete data; SSTable indexes point keys → byte offsets.[5][1]

- **Cluster mechanics**
  - Gossip protocol for node metadata exchange and membership.[3][1]
  - Fault tolerance via replication + hinted handoff + read repair; no single master node.[3][1]

- **Advanced features**
  - Storage‑Attached Indexes (SAI) for global secondary indexes on columns (slower than PK lookups, but avoid some denorm).[1]
  - Materialized Views for automatic denormalized tables maintained by Cassandra.[1]
  - Integration with Elasticsearch/Solr (e.g., Lucene index) for search workloads.[2][1]

***

## Common Trade‑offs + Example Questions

### Key trade‑offs

- **Availability & scale vs consistency & expressiveness**
  - Highly available, partition‑tolerant, linearly scalable; but limited transactions (partition‑local), no joins, and essentially query‑driven schema only.[2][3]

- **Query‑driven modeling vs flexibility**
  - You design tables per query; ad‑hoc queries or changing access patterns later can be expensive or impossible without new tables and backfills.[3][1]

- **Denormalization & duplication vs write complexity**
  - Multiple tables (views) denormalize same data to support different queries; write paths become more complex and heavier.[2][1]

- **Large partitions vs many tiny partitions**
  - Huge partitions hurt performance (compaction, read latency); too many tiny partitions increase overhead and metadata.[1]

### Example interview questions

- How would you model Discord‑style channel messages in Cassandra, and how would you handle very active channels (hot partitions)?[1]
- Design Ticketmaster‑style ticket browsing using Cassandra; how do you avoid one event partition becoming a hotspot?[1]
- Explain how Cassandra’s commit log, memtables, and SSTables work together to ensure durability and performance.[5][1]
- When would you pick Cassandra over MongoDB or a relational database, and what limitations must you call out to the interviewer?[6][3][2]

***

## Use Cases

- **High‑write, time‑ordered data**
  - Messaging, activity feeds, IoT telemetry, logging, metrics where writes are massive and predictable, and queries are strongly keyed by partition (e.g., per user, per device, per channel).[3][2]

- **Multi‑region, always‑on services**
  - Global user‑facing systems needing low latency and HA via multi‑DC replication and tunable consistency.[7][2]

- **Query‑driven denormalized stores**
  - Pre‑computed timelines, user dashboards, and counters with last‑write‑wins semantics and eventual consistency.[3][1]

- **Real‑world**
  - Many companies use Cassandra for telecom, financial analytics, event logging, and recommendation features due to its linear scalability and availability profile.[7][2]

***

## Cheat‑sheet, Q&A Style

- **Q: What kind of database is Cassandra?**  
  - A: A distributed, wide‑column NoSQL store optimized for write‑heavy, large‑scale, highly available workloads.[2][3]

- **Q: How is the primary key structured?**  
  - A: `PRIMARY KEY ((partition_key1, ...), clustering_key1, ...)`; partition key picks node/partition, clustering keys define on‑disk order within that partition.[1]

- **Q: Why is Cassandra “query‑driven” for modeling?**  
  - A: Because you must design tables so each important query is served from a single partition without joins or cross‑partition scans; schema follows queries, not entities.[3][1]

- **Q: How does Cassandra achieve durability?**  
  - A: Writes go to the commit log (WAL) and then memtables; memtables flush to SSTables; on restart, WAL is replayed if needed.[5][1]

- **Q: How do replication and consistency work?**  
  - A: Data is replicated to `replication_factor` nodes per keyspace; each operation specifies a consistency level (e.g., ONE, QUORUM, ALL, LOCAL_QUORUM) balancing latency vs consistency.[2][1]

- **Q: When is Cassandra a poor choice?**  
  - A: When you need strong ACID across many rows/partitions, complex ad‑hoc queries, heavy aggregations, or frequent schema‑agnostic exploration.[8][3]

- **Q: How do you handle hot partitions?**  
  - A: Use composite partition keys with buckets (e.g., `(channel_id, bucket)` or `(event_id, section_id)`), or re‑model access patterns to spread load.[3][1]

- **Q: What are SAI and materialized views?**  
  - A: Storage‑Attached Indexes provide global secondary indexes on columns; MVs let Cassandra maintain denormalized tables automatically from a base table.[1]

***

## Data Structures & Algorithms

- **Data structures**
  - Commit log (append‑only WAL segments) for durability.[5][1]
  - Memtables: in‑memory sorted maps (often skip lists or similar) per table.[9][5]
  - SSTables: immutable, sorted on‑disk files with per‑file indexes mapping keys → offsets.[5][1]
  - Token ring and partitioner hash function (often Murmur3) for partition assignment across nodes.[4][3]

- **Algorithms**
  - Log‑structured merge (LSM) tree write path: append logs, write to memtables, flush to SSTables, then periodically compact.[5][3]
  - Compaction algorithms to merge SSTables, enforce tombstones, and reduce read amplification.[5][1]
  - Gossip protocol for membership and state dissemination between nodes.[3][1]
  - Tunable consistency / quorum reads & writes (R, W, N) in the Dynamo‑style replication model.[10][3]

***

## Tools / Frameworks / Software

- **Core**
  - Apache Cassandra (OSS), DataStax Enterprise (commercial distribution).[2][3]

- **Ecosystem / ops**
  - Drivers for Java, Python, Go, etc., implementing token‑aware routing and load balancing.[3]
  - Management/monitoring: DataStax OpsCenter, Prometheus/Grafana dashboards, Reaper for repair automation.[2]
  - Integration: Kafka + Debezium/CDC pipelines, Spark for analytics, Elasticsearch/Solr for search indexing.[2][1]

***

## Big Tech Blog References (Cassandra)

- **Netflix – Global data platform on Cassandra**  
  - Netflix’s engineering blog has multiple articles describing how they built globally distributed services (e.g., viewing history, personalization, streaming metadata) on top of Cassandra for high write throughput and multi‑region availability.[1][2]
  - They highlight using Cassandra’s tunable consistency and multi‑DC replication to stay available during region failures and to serve low‑latency reads close to users, while offloading heavy analytics to separate systems.[2][1]

- **Instagram / Facebook – Messaging & feeds**  
  - Public talks and write‑ups describe Instagram using Cassandra as a backing store for time‑ordered data such as feeds and messaging, leveraging its wide‑row model (partition key + clustering on timestamp) and high write scalability.[3][1]
  - They emphasize query‑driven modeling (e.g., one partition per user or per thread) and accepting eventual consistency to handle massive fan‑out and fan‑in workloads.[4][3]

- **eBay / Uber‑style workloads**  
  - Case‑study collections list eBay, Uber and other large companies using Cassandra for mission‑critical workloads like metrics, logging, and customer‑facing features where write load and global availability are primary constraints.[1][2]
  - These teams call out that Cassandra allowed them to scale linearly by adding nodes, but forced them to invest heavily in data modeling discipline, capacity planning, and operational tooling (repair, compaction tuning, backup/restore).[3][1]

- **Managed‑service vendors (DataStax, Instaclustr, managed Cassandra)**  
  - Vendor blogs aggregate real‑world customer stories: moving from single‑region RDBMS to Cassandra clusters to eliminate single‑point failures and handle bursty workloads (e.g., marketing spikes, Black Friday), often reporting large gains in write throughput and uptime.[2][1]

These are the kinds of concrete references you can drop in an interview when asked “who actually uses Cassandra and why did it help them,” while tying back to the trade‑offs you already captured (query‑driven modeling, eventual consistency, high write scale, operational complexity).[1][3]


## Concise Summary: Important Terms & Keywords

- Wide‑column store, keyspace, table, row, column.[3][1]
- Primary key, partition key, clustering key; composite keys and partition size.[1]
- Query‑driven data modeling, denormalization, single‑partition queries.[3][1]
- Replication factor, `SimpleStrategy`, `NetworkTopologyStrategy`, tunable consistency (ONE, QUORUM, ALL, LOCAL_QUORUM).[2][1]
- Commit log, memtable, SSTable, compaction, tombstones, SSTable index.[5][1]
- Gossip, token ring, partitioner, virtual nodes.[4][3][1]
- Storage‑Attached Index (SAI), materialized views, search indexing integration.[1]

[1](https://www.hellointerview.com/learn/system-design/core-concepts/data-modeling)
[2](https://dzone.com/articles/top-nosql-databases-and-use-cases)
[3](https://blazeclan.com/blog/dive-deep-types-nosql-databases/)
[4](https://www.geeksforgeeks.org/system-design/is-consistent-hashing-used-in-sharding/)
[5](https://dbfromzero.com/write_ahead_log.html)
[6](https://blog.idineshkrishnan.com/2024/03/decoding-nosql-dynamodb-mongodb-and.html)
[7](https://www.instaclustr.com/education/nosql-database/nosql-databases-types-use-cases-and-8-databases-to-try/)
[8](https://www.altexsoft.com/blog/nosql-databases/)
[9](https://vivekbansal.substack.com/p/database-internals-write-ahead-logging)
[10](https://www.yugabyte.com/blog/dynamodb-vs-mongodb-vs-cassandra-for-fast-growing-geo-distributed-apps/)
[11](https://www.hellointerview.com)